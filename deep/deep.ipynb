{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN ( 다층 인공 신경망) 에 대해 알아보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'https://drive.google.com/uc?id=1i2lOnEyYJrKfiSHKqfVA4Zke1DEBSxuB' width = 1000, height = 500>\n",
    "\n",
    "일반적으로 은닉층은 100 - 50 - 20 .... 이런 식으로 점점 줄어들게 설정하는 것이 좋다고 알려져있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 순전파와 __**`역전파`**__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 순전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림과 같이 ANN은 첫 번째 줄의 선처럼 각 노드들이 다음 노드에 쭉쭉쭉 연결되는 그림과 같다.\n",
    "\n",
    "제일 윗 줄의 노드들만 생각해서 이해해보자.\n",
    "\n",
    "1. 입력층 -> 은닉층 1\n",
    "\n",
    "입력 값을 받고, 가중치 (W1) 와 입력값을 곱하고 편향 (b) 을 더해 가중합을 만들고, 그 가중합을 활성화 함수(sigmoid, Relu, selu...)에 넣어 활성화 값 __a__ 를 만든다.\n",
    "\n",
    "$W_1 \\times X + b = Z^{(L)}$, $activation(Z^{(L)}) = a^{(L)}$ (**`주의`** ! 여기서 (L) 은 지수가 아닌 순번)\n",
    "\n",
    "2. 은닉층 1 -> 은닉층 2\n",
    "\n",
    "은닉층 1 한 노드에 있는 활성화 값에 가중치와 편향을 더하고 가중합을 만들어 활성화 함수에 넣어 은닉층 2 로 간다.\n",
    "\n",
    "$W_{1-2} \\times a^{(L)} + b = Z^{(L+1)}$, $activation(Z^{(L+1)}) = a^{(L+1)}$\n",
    "\n",
    "3. 은닉층 2 -> 출력층\n",
    "\n",
    "마지막으로 은닉층 2 에서 활성화 값에 가중치와 편향을 더하고 가중합을 만들어 활성화 함수에 넣고 결과값과 차이를 비교한다.\n",
    "\n",
    "비용 함수를 만들어 오차가 얼마인지 계산한다.\n",
    "\n",
    "$(a^{(L+1)} - y_i)^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **`역전파`**\n",
    "\n",
    "역전파는 $(a^{(L+1)} - y_i)^2$ 의 최소 즉 비용함수의 최솟값을 얻기위한 파라미터들(가중치, 편향)을 구하는 것을 목적으로 한다.\n",
    "\n",
    "비용함수는 2차 함수이기 때문에 convex 하고 최소를 구할 수 있고. 그 최소는 기울기가 0 인 지점이다.\n",
    "\n",
    "__경사하강법__\n",
    "\n",
    "비용함수에서의 도함수가 0 이 되도록하는 기울기를 지속적으로 구하는 것을 경사하강법으로 해결한다. 식은 아래와 같다.\n",
    "\n",
    "$W_{after} = W_{before} - lr \\times \\frac{dCost}{dW_i}$\n",
    "\n",
    "갱신하기 전 가중치에서 학습률을 곱하고 비용함수를 가중치에대해 편미분한 값을 빼줌으로서 가중치를 갱신한다.\n",
    "\n",
    "개인적으로 왜 저기에 - 가 있는건지 궁금했었는데, 이유를 해석해보면\n",
    "\n",
    "<img src = 'https://drive.google.com/uc?id=1NewLCU-gxfeX0PfnAsyj6wWEO6XJWtLB' width = 1000, height = 400>\n",
    "\n",
    "x 축을 기울기로 두고 y 축을 오차라고 했을 때, \n",
    "\n",
    "1. 기울기가 위의 그림처럼 음수에 위치해 있다면,\n",
    "\n",
    "가중치가 커져야 global optimum 에 위치할 수 있기 때문에 지금의 가중치에서 -(lr $\\times$ - 편미분기울기) 라서 \n",
    "\n",
    "-$\\times$ - = + 가 돼 결국 지금 가중치가 커져서 global optimum 에 도달할 수 있게하고..\n",
    "\n",
    "2. 기울기가 양수에 있다면, \n",
    "\n",
    "global optimum 에서 우측에 위치하기 때문에 현재 가중치가 작아져야 하니까\n",
    "\n",
    " -(lr $\\times$ 편미분기울기) 라서 지금의 가중치에서 __-__ 빼기 때문에 global optimum으로 도달할 수 있게한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일반화해서 계산을 한 번 해보자. (실습)\n",
    "\n",
    "<img src = 'https://drive.google.com/uc?id=1Liq8OuzuohT5d2RyJQyLcgmzyBkZ3AR5' width = 900, height = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A3 의 활성화 값은 \n",
    "\n",
    "- $a^{(A3)} = activation(V1 \\times a^{(A1 node)} + V2 \\times a^{(A2 node)} + b)$\n",
    "\n",
    "A3 의 가중합은 \n",
    "\n",
    "- $Z^{(A3)} = V1 \\times a^{(A1 node)} + V2 \\times a^{(A2 node)}$\n",
    "\n",
    "비용함수를 구해보면..\n",
    "\n",
    "Error = $(a^{(A3)} - y_i)^2$ 이고 여기에서 오차값이 나와 갱신을 한다고 했을 때 \n",
    "\n",
    "갱신해야 하는 가중치는 V1 과 V2 이므로 $\\frac{dError}{dV1}$ 과 $\\frac{dError}{dV2}$ 를 진행한다.\n",
    "\n",
    "$\\frac{dError}{dV1}$ 은 바로 계산할 수 없으므로 Chain rule 을 이용한다.\n",
    "\n",
    "$\\frac{dError}{dV1}$ =  $\\frac{dError}{da^{(A3)}} \\times \\frac{da^{(A3)}}{dZ^{(A3)}} \\times \\frac{dZ^{(A3)}}{dV1}$\n",
    "\n",
    "편미분으로 위의 식처럼 V1 과 V2 에 대해서 각각 편미분 해줘야한다. (위의 식은 V1에 대해서만 편미분 했음.)\n",
    "\n",
    "그 후 기울기를 갱신하게 된다. (**`경사하강법`**)\n",
    "\n",
    "$V1_{after} = V1_{before} - lr \\times \\frac{dError}{dV1}$\n",
    "\n",
    "__위의 과정은 컴퓨터가 계산함.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`모델`** 만들기.\n",
    "\n",
    "모델을 만들기 전에는 데이터에 대해서 다양한 전처리를 해야한다.\n",
    "\n",
    "train_test_split , standardscaler... 기타 등등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential() # 모델을 구축하겠다.\n",
    "model2.add(Dense(128)) # 은닉층의 노드 수 점점 줄어드는 모양으로 설정하는 것이 일반적으로 좋다고 알려짐.\n",
    "model2.add(BatchNormalization()) # 다음에 정리할 BatchNormalization 일단은 넘어가도록 하자.\n",
    "model2.add(Activation('relu')) # 가중합을 Relu 활성화 함수에 넣겠다.\n",
    "model2.add(Dense(64)) # 128 보다 작게 설정한 모습.\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(1, activation = 'sigmoid')) # 이진 분류이므로 노드수 1, 활성화 함수로는 시그모이드(sigmoid)\n",
    "\n",
    "model2.compile(optimizer = 'adam', # 어쩔 아담. RMSProp(눈치보면서 lr 조절)과 Momentum (관성) 을 섞었음. 얘가 성능이 좋다고 알려짐\n",
    "               loss = 'binary_crossentropy', # 손실함수 => 활성화 함수가 시그모이드(threshold 0.5 를 기준으로 0 , 1 을 분류)이므로 binary_crossentropy\n",
    "               metrics = ['accuracy']) # 분류 평가지표 넣기.\n",
    "\n",
    "results = model2.fit(X_train_scaled, y_train, epochs = 10)\n",
    "                     # 모델을 훈련할거야, 검증데이터로 (X_test_scaled, y_test) 쓸거고 10 번 돌아간다. 파라미터(가중치, 편향) 갱신을 10 번\n",
    "                                                                                                    # 계산은 optimizer = 'adam' 아담이 해줄거야"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28))) # 사진 데이터의 경우 몇 by 몇인지 알고 그것을 쭉 일자로 펴줘야함. 그래서 flatten 을 이용한 것.\n",
    "model.add(Dense(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=4)) # 입력벡터의 차원수가 4개다. # 사진데이터가 아닌 경우 알아서 해결하는 듯 \n",
    "                      \n",
    "                     # X_train[0] = [0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation = 'softmax')) # \n",
    "\n",
    "model.compile(optimizer = 'adam', \n",
    "               loss = 'binary_crossentropy',\n",
    "               metrics = ['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('deeplearning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c8f46a1faa65f2d37ffc4d337c572e84eb0a0eb34c464f3ede6312da8342375"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
